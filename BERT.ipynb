{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgorden-ggm\u001b[0m (\u001b[33mgt\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Gorde/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, TFTrainingArguments, TFAutoModelForSequenceClassification, TFTrainer\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "\n",
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.9)\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "wandb.login(key = '5d7ca03a263dd16d3b863633f7ddf7900842d17c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depression_dataset = pd.read_csv('Dataset/dataset.csv')\n",
    "df_depression_dataset.dropna(inplace = True)\n",
    "\n",
    "df_train, df_test = train_test_split(df_depression_dataset, test_size = 0.2, random_state = 1)\n",
    "df_train, df_valid = train_test_split(df_train, test_size = 0.1, random_state = 1)\n",
    "\n",
    "X_train = df_train['clean_text'].values.tolist()\n",
    "y_train = df_train['is_depression'].values.tolist()\n",
    "\n",
    "X_valid = df_valid['clean_text'].values.tolist()\n",
    "y_valid = df_valid['is_depression'].values.tolist()\n",
    "\n",
    "X_test = df_test['clean_text'].values.tolist()\n",
    "y_test = df_test['is_depression'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(X_train, truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(X_valid, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(X_test, truncation=True, padding=True)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings),y_train))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((dict(valid_encodings),y_valid))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings),y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\OneDrive\\Study\\TA\\Final\\PJ\\wandb\\run-20221209_181721-22hy60py</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/gt/huggingface/runs/22hy60py\" target=\"_blank\">./results</a></strong> to <a href=\"https://wandb.ai/gt/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TFTrainingArguments(\n",
    "    output_dir = './results',         \n",
    "    num_train_epochs = 3,              \n",
    "    per_device_train_batch_size = 4,  \n",
    "    per_device_eval_batch_size = 8,           \n",
    "    weight_decay = 0.01,              \n",
    "    logging_dir = './logs',           \n",
    "    logging_steps = 10\n",
    ")\n",
    "\n",
    "with training_args.strategy.scope():\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "trainer = TFTrainer(\n",
    "    model = model,                         \n",
    "    args = training_args,                \n",
    "    train_dataset = train_dataset,         \n",
    "    eval_dataset = valid_dataset           \n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x220dbd63760>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x220dbd63760>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[782   0]\n",
      " [  0 764]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, prediction[1])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_save_directory = \"Model\\model_1\"\n",
    "tokenizer.save_pretrained(tf_save_directory)\n",
    "model.save_pretrained(tf_save_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aaf24b35d1082babedaf1ec429f80fde0c6b24460ed3b4a026756309d6b1151f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
